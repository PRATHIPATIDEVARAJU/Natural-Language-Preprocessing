{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44e5bec9-a8b8-451b-81d2-3d3f6505ba3d",
   "metadata": {},
   "source": [
    "# Nature Language Processing ( NLP )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd4646c-337e-4a2d-a125-6ba789979099",
   "metadata": {},
   "source": [
    "- Natural Language Processing (NLP) is a fascinating field that sits at the intersection of artificial intelligence, linguistics, and computer science.\n",
    "\n",
    "- It deals with the interaction between computers and humans using natural language.\n",
    "\n",
    "- Here's a basic introduction to NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b9fd7-5bc4-4d71-8380-80e837e578a8",
   "metadata": {},
   "source": [
    "**Definition:** NLP is a branch of artificial intelligence that enables computers to understand, interpret, and generate human language in a way that is both meaningful and useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90569b4d-5c0b-46ba-902e-0a6e12a441e9",
   "metadata": {},
   "source": [
    "# ðŸŒ Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is a field of **Artificial Intelligence (AI)** that focuses on enabling computers to understand, interpret, process, and generate human language. It is a multidisciplinary domain that combines **linguistics, computer science, and machine learning** to bridge the gap between human communication and machine understanding.  \n",
    "\n",
    "The primary goal of NLP is to allow computers to process large amounts of **textual or spoken data** in a way that is both meaningful and useful. It involves various **computational techniques** to analyze and manipulate human language, making it possible for machines to perform tasks such as **language translation, speech recognition, sentiment analysis, text summarization, chatbots, and more.**  \n",
    "\n",
    "NLP is an essential component of many **real-world applications**, including **virtual assistants like Siri and Alexa, search engines like Google, spam detection in emails, and customer support automation.** It is widely used in fields such as **healthcare, finance, e-commerce, and education** to enhance user experience and automate communication-based tasks.  \n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“– Definition of Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing (NLP) is a **branch of artificial intelligence** that focuses on the interaction between **computers and human languages.** It enables computers to process and analyze large amounts of natural language data in a way that allows them to **understand, interpret, generate, and respond** to human communication effectively.  \n",
    "\n",
    "At its core, NLP involves the following key tasks:  \n",
    "\n",
    "### ðŸ”¹ Understanding (Natural Language Understanding - NLU)\n",
    "- Recognizing and processing **human language** in a structured format  \n",
    "- Identifying **syntax, semantics, and context** within text  \n",
    "- Converting **unstructured text data** into machine-readable structured information  \n",
    "\n",
    "### ðŸ”¹ Processing (Natural Language Processing - NLP)\n",
    "- Tokenization, stemming, lemmatization, and parsing of text  \n",
    "- Text classification and sentiment analysis  \n",
    "- Information extraction and named entity recognition  \n",
    "\n",
    "### ðŸ”¹ Generation (Natural Language Generation - NLG)\n",
    "- Generating meaningful responses in human language  \n",
    "- Text summarization and machine translation  \n",
    "- Conversational AI and chatbots  \n",
    "\n",
    "### ðŸ’¡ Key Characteristics of NLP:\n",
    "âœ… Deals with **spoken and written text**  \n",
    "âœ… Involves **computational linguistics** and AI techniques  \n",
    "âœ… Uses **statistical models and machine learning** for text analysis  \n",
    "âœ… Aims to make **human-computer interactions** seamless  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Why is NLP Important?\n",
    "In todayâ€™s digital world, vast amounts of **textual data** are generated every second, from social media posts and customer reviews to emails and online articles. Manually analyzing such data is impossible, which is why NLP plays a crucial role in **automating** text processing and understanding.  \n",
    "\n",
    "#### ðŸ”¸ Applications of NLP:\n",
    "âœ… **Search Engines** â€“ Google, Bing, and Yahoo use NLP to provide accurate search results  \n",
    "âœ… **Virtual Assistants** â€“ Siri, Alexa, and Google Assistant rely on NLP for voice recognition and response generation  \n",
    "âœ… **Spam Detection** â€“ Email services use NLP to filter spam and classify emails  \n",
    "âœ… **Sentiment Analysis** â€“ Businesses use NLP to analyze customer reviews and feedback  \n",
    "âœ… **Language Translation** â€“ Google Translate and other tools translate languages using NLP models  \n",
    "âœ… **Chatbots** â€“ Automated customer service and AI-driven chat systems  \n",
    "\n",
    "---\n",
    "\n",
    "### ðŸš€ Conclusion\n",
    "NLP is one of the most exciting and rapidly evolving fields in AI. It empowers machines to understand and process human language, making it easier for businesses and individuals to communicate efficiently with AI-driven systems.  \n",
    "\n",
    "The combination of **deep learning, machine learning, and linguistic rules** has led to significant advancements in NLP, enabling applications such as **real-time translation, intelligent chatbots, and personalized recommendations.**  \n",
    "\n",
    "As NLP continues to advance, it is expected to **revolutionize communication, automate repetitive tasks, and make human-computer interactions more seamless than ever before.** ðŸš€ðŸ’¡  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4bd7fb-1811-4ff3-a359-aea834dafbae",
   "metadata": {},
   "source": [
    "## NLP Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66a8572-9b9d-47fb-b3c5-718aae3c7532",
   "metadata": {},
   "source": [
    "- Stemming\n",
    "- Lemmatization\n",
    "- Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- Bag of Words\n",
    "- Word2Vec\n",
    "- Word Embeddings\n",
    "- Skip Grams\n",
    "- CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c0cec3-5aaa-4e2c-a1cd-83038e60cf39",
   "metadata": {},
   "source": [
    "# ðŸ” Natural Language Processing (NLP) Techniques\n",
    "\n",
    "NLP consists of various techniques that help machines process and understand human language efficiently. These techniques range from basic text preprocessing methods to advanced word representation models.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ 1. Stemming\n",
    "**Stemming** is the process of **reducing words to their root form** by removing suffixes. It does not necessarily produce a meaningful word but helps in reducing **dimensionality** in text data.\n",
    "\n",
    "### ðŸ”¹ Example:\n",
    "| Word | Stemmed Form |\n",
    "|------|-------------|\n",
    "| Playing | Play |\n",
    "| Running | Run |\n",
    "| Studies | Studi |\n",
    "\n",
    "**Algorithm Used:** Porter Stemmer, Lancaster Stemmer, Snowball Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56ad2b8d-ffb2-4261-97bc-2eedd6225d0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "studi\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "print(ps.stem(\"running\"))  # Output: run\n",
    "print(ps.stem(\"studies\"))  # Output: studi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca7b16-8591-444b-99e0-7087788e0975",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## ðŸ“Œ 2. Lemmatization\n",
    "\n",
    "- Lemmatization is similar to stemming but ensures that the reduced word is **linguistically correct**. It uses a **dictionary-based approach** to convert words to their base form.\n",
    "- Lemmatization is a text normalization technique in **Natural Language Processing (NLP)** that reduces words to their **base or dictionary form (lemma)**, considering the **context and meaning**.\n",
    "\n",
    "### ðŸ”¹ Example:\n",
    "| Word |\tLemmatized Form |\n",
    "| ------| ---------------------- |\n",
    "|  Playing |\tPlay    |\n",
    "| Studies\t| Study  |\n",
    "| Better\t| Good |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27ef0418-aab2-444c-b73d-281b35a4a652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running\n",
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"running\"))  # Output: running\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # Output: good\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773c3f0-3a0e-40ef-9b5f-88e5487a0303",
   "metadata": {},
   "source": [
    "### ðŸ“Œ 3. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "**TF-IDF** is a statistical measure used to evaluate the **importance of a word** in a document relative to a collection of documents (corpus). It assigns a weight to each word based on its frequency in a document and its rarity in the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5401ebc6-2bfe-4a67-ab87-ebc32353cf08",
   "metadata": {},
   "source": [
    "## ðŸ”¹ Formula:\n",
    "\n",
    "## ðŸ”¢ **Mathematical Formula**\n",
    "\n",
    "### **ðŸ“Œ Term Frequency (TF)**\n",
    "\n",
    "$$\n",
    "TF = \\frac{\\text{Number of times a word appears in a document}}{\\text{Total words in the document}}\n",
    "$$\n",
    "\n",
    "### **ðŸ“Œ Inverse Document Frequency (IDF)**\n",
    "\n",
    "\n",
    "$$\n",
    "IDF = \\log \\left( \\frac{\\text{Total number of documents}}{\\text{Number of documents containing the word}} \\right)\n",
    "$$\n",
    "\n",
    "### **ðŸ“Œ TF-IDF Score**\n",
    "$$\n",
    "TF-IDF = TF \\times IDF\n",
    "$$\n",
    "\n",
    "**Application**: Search engines, text ranking, document similarity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dbdb2b0-77bb-4f9b-b59e-a2bee61ff74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'is' 'language' 'love' 'natural' 'nlp' 'processing']\n",
      "[[0.4472136  0.4472136  0.4472136  0.         0.4472136  0.\n",
      "  0.4472136 ]\n",
      " [0.         0.         0.         0.70710678 0.         0.70710678\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"Natural Language Processing is amazing\", \"I love NLP\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names_out())  # Output: ['amazing' 'language' 'love' 'natural' 'nlp' 'processing']\n",
    "print(tfidf_matrix.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69eab863-e524-4b11-aeb9-58cbd00c2733",
   "metadata": {},
   "source": [
    "### ðŸ“Œ 4. Bag of Words (BoW)\n",
    "\n",
    "BoW is a technique that represents text as a **collection of words**, disregarding grammar and word order but keeping track of occurrences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d3b58a-4381-4ed4-ac8f-82d579cf4a22",
   "metadata": {},
   "source": [
    "### ðŸ”¹ Example:\n",
    "|  Sentence\t| Representation |\n",
    "|----------|----------------|\n",
    "|  NLP is awesome\t| NLP (1), is (1), awesome (1) |\n",
    "| I love NLP\t| I (1), love (1), NLP (1) |\n",
    "\n",
    "\n",
    "**Application**: Spam detection, text classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d71a23-cc3a-4e58-b6da-a85fa2ed409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazing' 'is' 'love' 'nlp']\n",
      "[[0 0 1 1]\n",
      " [1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform([\"I love NLP\", \"NLP is amazing\"])\n",
    "print(vectorizer.get_feature_names_out())  # Output: ['amazing' 'is' 'love' 'nlp']\n",
    "print(X.toarray())  # Word frequency matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2b46ae-fd83-46ad-ac92-817a57b043da",
   "metadata": {},
   "source": [
    "### ðŸ“Œ 5. Word2Vec\n",
    "\n",
    "Word2Vec is a **word embedding technique** that transforms words into **numerical vectors** while capturing their semantic meaning. It uses a **neural network** to learn relationships between words.\n",
    "\n",
    "**Two types of Word2Vec models:**\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)** â€“ Predicts a target word from surrounding words.\n",
    "\n",
    "2. **Skip-Gram** â€“ Predicts surrounding words from a target word.\n",
    "\n",
    "**Application:** Sentiment analysis, recommendation systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73267072-6323-4cc2-bc54-2960e95e1f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0724545e-03  4.7286271e-04  1.0206699e-02  1.8018546e-02\n",
      " -1.8605899e-02 -1.4233618e-02  1.2917745e-02  1.7945977e-02\n",
      " -1.0030856e-02 -7.5267432e-03  1.4761009e-02 -3.0669428e-03\n",
      " -9.0732267e-03  1.3108104e-02 -9.7203208e-03 -3.6320353e-03\n",
      "  5.7531595e-03  1.9837476e-03 -1.6570430e-02 -1.8897636e-02\n",
      "  1.4623532e-02  1.0140524e-02  1.3515387e-02  1.5257311e-03\n",
      "  1.2701781e-02 -6.8107317e-03 -1.8928028e-03  1.1537147e-02\n",
      " -1.5043275e-02 -7.8722071e-03 -1.5023164e-02 -1.8600845e-03\n",
      "  1.9076237e-02 -1.4638334e-02 -4.6675373e-03 -3.8754821e-03\n",
      "  1.6154874e-02 -1.1861792e-02  9.0324880e-05 -9.5074680e-03\n",
      " -1.9207101e-02  1.0014586e-02 -1.7519170e-02 -8.7836506e-03\n",
      " -7.0199967e-05 -5.9236289e-04 -1.5322480e-02  1.9229487e-02\n",
      "  9.9641159e-03  1.8466286e-02]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"NLP\", \"is\", \"amazing\"], [\"I\", \"love\", \"NLP\"]]\n",
    "model = Word2Vec(sentences, vector_size=50, window=5, min_count=1, workers=4)\n",
    "print(model.wv[\"NLP\"])  # Word vector for \"NLP\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc75e20f-450d-4021-be78-bab1f84783f8",
   "metadata": {},
   "source": [
    "## ðŸ“Œ 6. Word Embeddings\n",
    "\n",
    "Word embeddings are vector representations of words that **capture relationships** based on their meaning and usage in a corpus.\n",
    "\n",
    "**Popular Pre-trained Word Embeddings:** âœ… Word2Vec\n",
    "âœ… GloVe (Global Vectors for Word Representation)\n",
    "âœ… FastText\n",
    "\n",
    "These embeddings help NLP models **understand semantic similarity** between words.\n",
    "\n",
    "### ðŸ”¹ Example:\n",
    "\n",
    "- King - Man + Woman â‰ˆ Queen\n",
    "\n",
    "- Paris - France + Germany â‰ˆ Berlin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "233b66fc-5aa7-48f4-b358-c9f35cde95dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Load pre-trained Word2Vec embeddings (GloVe, FastText, etc.)\n",
    "# model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin\", binary=True)\n",
    "\n",
    "# Finding similar words\n",
    "# print(model.most_similar(\"king\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c11a8034-cf1f-46e5-b6ad-3936bdd1e1d7",
   "metadata": {},
   "source": [
    "## ðŸ“Œ 7. Skip-Gram Model\n",
    "\n",
    "Skip-Gram is an NLP technique used in **Word2Vec** that predicts **context words** given a target word. It learns word representations by maximizing the probability of **neighboring words** appearing together.\n",
    "\n",
    "**Working:**\n",
    "\n",
    "- Given a **center word**, predict its **surrounding words.**\n",
    "\n",
    "- Works well with **small datasets**.\n",
    "\n",
    "### ðŸ”¹ Example:\n",
    "\n",
    "If the sentence is `\"I love NLP and machine learning\"`\n",
    "\n",
    "- Target: `NLP`\n",
    "- Context: `[love, and]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2845f8f0-472e-410c-a420-0b2ecedb92ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('great', -0.01083916611969471), ('I', -0.02775035798549652), ('is', -0.05234673246741295), ('love', -0.111670583486557)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"I\", \"love\", \"NLP\"], [\"NLP\", \"is\", \"great\"]]\n",
    "model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, sg=1)  # sg=1 for Skip-Gram\n",
    "print(model.wv.most_similar(\"NLP\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3ea3a9-b2b5-46ec-9843-77b413b67847",
   "metadata": {},
   "source": [
    "## ðŸ“Œ 8. CBOW (Continuous Bag of Words)\n",
    "\n",
    "CBOW is another **Word2Vec** approach where the model predicts a **target word** from its **context words**.\n",
    "\n",
    "**Working:**\n",
    "\n",
    "- Given **context words,** predict the **center word.**\n",
    "\n",
    "- Works well with **large datasets**.\n",
    "\n",
    "### ðŸ”¹ Example:\n",
    "\n",
    "If the sentence is `\"I love NLP and machine learning\"`\n",
    "\n",
    "- Context: `[I, love, and, machine, learning]`\n",
    "- Target: `NLP`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca44b033-7681-489e-baed-361cbe4b537e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('powerful', -0.01083916611969471), ('I', -0.02775035798549652), ('is', -0.05234673246741295), ('love', -0.111670583486557)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"I\", \"love\", \"NLP\"], [\"NLP\", \"is\", \"powerful\"]]\n",
    "model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, sg=0)  # sg=0 for CBOW\n",
    "print(model.wv.most_similar(\"NLP\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e296d710-06c6-4c16-8784-599ec848680b",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Conclusion\n",
    "\n",
    "Natural Language Processing (NLP) uses various techniques to process and understand human language. From basic text preprocessing (stemming, lemmatization) to advanced word embeddings (Word2Vec, TF-IDF), NLP plays a crucial role in AI applications.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¥ Summary of NLP Techniques:\n",
    "\n",
    "| **Technique**      | **Purpose** |\n",
    "|-------------------|------------|\n",
    "| **Stemming**      | Reduce words to root form |\n",
    "| **Lemmatization** | Convert words to base form (dictionary-based) |\n",
    "| **TF-IDF**        | Assign importance to words based on frequency |\n",
    "| **Bag of Words (BoW)** | Convert text into word occurrence matrix |\n",
    "| **Word2Vec**      | Learn word relationships using vectors |\n",
    "| **Word Embeddings** | Represent words in dense numerical vectors |\n",
    "| **Skip-Gram**     | Predict context words from target word |\n",
    "| **CBOW**         | Predict target word from context |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Future of NLP\n",
    "\n",
    "NLP is a rapidly evolving field that continues to shape the future of AI-driven applications, making machines **smarter** in understanding human language. \n",
    "\n",
    "ðŸ”¹ **Key Applications of NLP:**\n",
    "- Chatbots & Virtual Assistants ðŸ¤–\n",
    "- Sentiment Analysis ðŸ§\n",
    "- Machine Translation ðŸŒ\n",
    "- Text Summarization ðŸ“„\n",
    "- Speech Recognition ðŸŽ™ï¸\n",
    "\n",
    "The advancements in **Deep Learning** and **Transformer models (e.g., BERT, GPT)** have significantly improved the accuracy of NLP models, paving the way for more **human-like language understanding**. âœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fb0d6c-0752-49ad-876e-9fba782499d4",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Applications, Advantages, and Disadvantages of NLP\n",
    "\n",
    "## ðŸ“Œ Application of NLP\n",
    "\n",
    "Natural Language Processing (NLP) has a wide range of applications across various industries:\n",
    "\n",
    "- **Sentiment Analysis** ðŸ§ â€“ Used in social media monitoring, customer feedback analysis, and market research.\n",
    "- **Machine Translation** ðŸŒ â€“ Automates language translation (e.g., Google Translate).\n",
    "- **Text Summarization** ðŸ“„ â€“ Generates concise summaries from large text documents.\n",
    "- **Question Answering** â“ â€“ Powers virtual assistants like Siri, Alexa, and chatbots.\n",
    "- **Healthcare** ðŸ¥ â€“ Helps in clinical documentation, medical chatbots, and disease prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Advantages of NLP\n",
    "\n",
    "NLP provides numerous benefits, enhancing efficiency and automation:\n",
    "\n",
    "- **Automation** ðŸ¤– â€“ Reduces manual effort in processing text data.\n",
    "- **Insight Extraction** ðŸ“Š â€“ Helps analyze large volumes of unstructured text for valuable insights.\n",
    "- **Personalization** ðŸŽ¯ â€“ Enables personalized recommendations (e.g., Netflix, Amazon).\n",
    "- **Language Translation** ðŸŒ â€“ Facilitates seamless communication across languages.\n",
    "\n",
    "---\n",
    "\n",
    "## âŒ Disadvantages of NLP\n",
    "\n",
    "Despite its advantages, NLP has some limitations:\n",
    "\n",
    "- **Data Quality and Bias** âš ï¸ â€“ NLP models can inherit biases present in training data.\n",
    "- **Privacy Concerns** ðŸ”’ â€“ Processing sensitive user data can raise ethical issues.\n",
    "- **Lack of Domain Specificity** ðŸ“š â€“ NLP models may struggle with industry-specific jargon.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸš€ Future Scope\n",
    "\n",
    "With advancements in **deep learning, transformers (e.g., BERT, GPT), and multimodal AI**, NLP is expected to revolutionize fields like **legal analysis, financial forecasting, and AI-driven content creation**. ðŸš€âœ¨\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70609209-9e49-4357-a7ec-f83738ed0877",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c6842aa-d590-4c9a-9222-6bfefb0bce96",
   "metadata": {},
   "source": [
    "# NLP Libraries Installzation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e71688-cffe-40e9-8f38-7b72e2561161",
   "metadata": {},
   "source": [
    "## 1) NLTK (Natural Language Toolkit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b742fc-4d8d-4b30-a76c-a1c24cf45453",
   "metadata": {},
   "source": [
    "- NLTK is a comprehensive library for building Python programs to work with human language data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cf95ba9-486c-4e65-a675-ce647ce36bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 764.3 kB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.5/1.5 MB 764.3 kB/s eta 0:00:02\n",
      "   -------------------- ------------------- 0.8/1.5 MB 714.3 kB/s eta 0:00:02\n",
      "   --------------------------- ------------ 1.0/1.5 MB 774.8 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 809.1 kB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.3/1.5 MB 809.1 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 804.2 kB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11ddfdc-02ec-4d91-8fc0-7813bf370904",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')  # This will install the missing resource\n",
    "nltk.download('punkt')  # Ensures that 'punkt' is installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "331bf76f-c4a8-43af-965b-ccdc2b71ed67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.data.path.append('C:\\\\Users\\\\devad\\\\AppData\\\\Roaming\\\\nltk_data')  # Ensure correct path\n",
    "nltk.download('punkt', force=True)  # Force re-download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f63b03-c632-497a-af11-48990f1bc764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', 'world', '!']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6cb5c5-1a73-497f-8c47-6ae358703d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\devad\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d0d383-a831-488c-9f4c-685e2691c7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'Language', 'Processing', 'is', 'fascinating', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Natural Language Processing is fascinating!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb731ce5-7295-4d9c-9698-d6016b3e5bf2",
   "metadata": {},
   "source": [
    "## 2. spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf4fb11-c440-4093-a9b3-0da4aaa8c776",
   "metadata": {},
   "source": [
    "- spaCy is an open-source library for advanced NLP in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8da6e513-54c0-4bcb-8af6-cfedaa3cf3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.1-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (75.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.20.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl.metadata (9.3 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Downloading spacy-3.8.4-cp312-cp312-win_amd64.whl (11.8 MB)\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/11.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.8 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/11.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.8 MB 558.9 kB/s eta 0:00:21\n",
      "   - -------------------------------------- 0.5/11.8 MB 558.9 kB/s eta 0:00:21\n",
      "   -- ------------------------------------- 0.8/11.8 MB 610.3 kB/s eta 0:00:19\n",
      "   -- ------------------------------------- 0.8/11.8 MB 610.3 kB/s eta 0:00:19\n",
      "   --- ------------------------------------ 1.0/11.8 MB 585.1 kB/s eta 0:00:19\n",
      "   --- ------------------------------------ 1.0/11.8 MB 585.1 kB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 1.3/11.8 MB 588.8 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.3/11.8 MB 588.8 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.3/11.8 MB 588.8 kB/s eta 0:00:18\n",
      "   ----- ---------------------------------- 1.6/11.8 MB 559.2 kB/s eta 0:00:19\n",
      "   ----- ---------------------------------- 1.6/11.8 MB 559.2 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 1.8/11.8 MB 541.2 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 1.8/11.8 MB 541.2 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 1.8/11.8 MB 541.2 kB/s eta 0:00:19\n",
      "   ------- -------------------------------- 2.1/11.8 MB 543.7 kB/s eta 0:00:18\n",
      "   -------- ------------------------------- 2.4/11.8 MB 556.9 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.4/11.8 MB 556.9 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.4/11.8 MB 556.9 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.4/11.8 MB 556.9 kB/s eta 0:00:17\n",
      "   -------- ------------------------------- 2.6/11.8 MB 531.7 kB/s eta 0:00:18\n",
      "   -------- ------------------------------- 2.6/11.8 MB 531.7 kB/s eta 0:00:18\n",
      "   --------- ------------------------------ 2.9/11.8 MB 537.7 kB/s eta 0:00:17\n",
      "   --------- ------------------------------ 2.9/11.8 MB 537.7 kB/s eta 0:00:17\n",
      "   --------- ------------------------------ 2.9/11.8 MB 537.7 kB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 3.1/11.8 MB 518.4 kB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 3.1/11.8 MB 518.4 kB/s eta 0:00:17\n",
      "   ---------- ----------------------------- 3.1/11.8 MB 518.4 kB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 3.4/11.8 MB 505.8 kB/s eta 0:00:17\n",
      "   ----------- ---------------------------- 3.4/11.8 MB 505.8 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 3.7/11.8 MB 513.2 kB/s eta 0:00:16\n",
      "   ------------ --------------------------- 3.7/11.8 MB 513.2 kB/s eta 0:00:16\n",
      "   ------------- -------------------------- 3.9/11.8 MB 525.5 kB/s eta 0:00:15\n",
      "   ------------- -------------------------- 3.9/11.8 MB 525.5 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 4.2/11.8 MB 530.9 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 4.2/11.8 MB 530.9 kB/s eta 0:00:15\n",
      "   -------------- ------------------------- 4.2/11.8 MB 530.9 kB/s eta 0:00:15\n",
      "   --------------- ------------------------ 4.5/11.8 MB 523.2 kB/s eta 0:00:15\n",
      "   --------------- ------------------------ 4.5/11.8 MB 523.2 kB/s eta 0:00:15\n",
      "   ---------------- ----------------------- 4.7/11.8 MB 528.2 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 4.7/11.8 MB 528.2 kB/s eta 0:00:14\n",
      "   ---------------- ----------------------- 5.0/11.8 MB 534.5 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 5.0/11.8 MB 534.5 kB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 5.2/11.8 MB 539.4 kB/s eta 0:00:13\n",
      "   ----------------- ---------------------- 5.2/11.8 MB 539.4 kB/s eta 0:00:13\n",
      "   ------------------ --------------------- 5.5/11.8 MB 541.2 kB/s eta 0:00:12\n",
      "   ------------------- -------------------- 5.8/11.8 MB 548.8 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 5.8/11.8 MB 548.8 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.0/11.8 MB 553.4 kB/s eta 0:00:11\n",
      "   -------------------- ------------------- 6.0/11.8 MB 553.4 kB/s eta 0:00:11\n",
      "   --------------------- ------------------ 6.3/11.8 MB 560.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 6.3/11.8 MB 560.0 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 6.6/11.8 MB 564.0 kB/s eta 0:00:10\n",
      "   ----------------------- ---------------- 6.8/11.8 MB 569.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 6.8/11.8 MB 569.1 kB/s eta 0:00:09\n",
      "   ----------------------- ---------------- 6.8/11.8 MB 569.1 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.1/11.8 MB 564.3 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.1/11.8 MB 564.3 kB/s eta 0:00:09\n",
      "   ------------------------ --------------- 7.3/11.8 MB 569.8 kB/s eta 0:00:08\n",
      "   ------------------------ --------------- 7.3/11.8 MB 569.8 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 7.6/11.8 MB 571.5 kB/s eta 0:00:08\n",
      "   ------------------------- -------------- 7.6/11.8 MB 571.5 kB/s eta 0:00:08\n",
      "   -------------------------- ------------- 7.9/11.8 MB 576.5 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.1/11.8 MB 580.5 kB/s eta 0:00:07\n",
      "   --------------------------- ------------ 8.1/11.8 MB 580.5 kB/s eta 0:00:07\n",
      "   ---------------------------- ----------- 8.4/11.8 MB 585.0 kB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 8.4/11.8 MB 585.0 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 8.7/11.8 MB 586.7 kB/s eta 0:00:06\n",
      "   ----------------------------- ---------- 8.7/11.8 MB 586.7 kB/s eta 0:00:06\n",
      "   ------------------------------ --------- 8.9/11.8 MB 590.3 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 9.2/11.8 MB 593.6 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 9.2/11.8 MB 593.6 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 9.4/11.8 MB 597.4 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 9.4/11.8 MB 597.4 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 9.7/11.8 MB 599.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 10.0/11.8 MB 602.7 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 10.0/11.8 MB 602.7 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 10.2/11.8 MB 607.2 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 610.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 610.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 610.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 610.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 610.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 610.9 kB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 10.5/11.8 MB 610.9 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 10.7/11.8 MB 578.0 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.7/11.8 MB 578.0 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 10.7/11.8 MB 578.0 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.0/11.8 MB 576.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.0/11.8 MB 576.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.0/11.8 MB 576.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 11.0/11.8 MB 576.1 kB/s eta 0:00:02\n",
      "   -------------------------------------- - 11.3/11.8 MB 564.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.8 MB 564.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.3/11.8 MB 564.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 560.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.8 MB 560.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.8/11.8 MB 560.1 kB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-win_amd64.whl (122 kB)\n",
      "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
      "Downloading pydantic_core-2.27.2-cp312-cp312-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 730.2 kB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 730.2 kB/s eta 0:00:03\n",
      "   --------------- ------------------------ 0.8/2.0 MB 699.0 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 709.1 kB/s eta 0:00:02\n",
      "   --------------------- ------------------ 1.0/2.0 MB 709.1 kB/s eta 0:00:02\n",
      "   -------------------------- ------------- 1.3/2.0 MB 729.7 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 729.4 kB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.6/2.0 MB 729.4 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.8/2.0 MB 745.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.0/2.0 MB 741.8 kB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.6 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 262.1/632.6 kB ? eta -:--:--\n",
      "   -------------------------------------- 632.6/632.6 kB 791.6 kB/s eta 0:00:00\n",
      "Downloading thinc-8.3.4-cp312-cp312-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   -------------- ------------------------- 0.5/1.5 MB 419.4 kB/s eta 0:00:03\n",
      "   --------------------- ------------------ 0.8/1.5 MB 168.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 168.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 168.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 0.8/1.5 MB 168.6 kB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 204.6 kB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 1.0/1.5 MB 204.6 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 1.3/1.5 MB 247.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 269.0 kB/s eta 0:00:00\n",
      "Downloading typer-0.15.1-py3-none-any.whl (44 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 1.0 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 931.2 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 931.2 kB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.0/6.3 MB 838.4 kB/s eta 0:00:07\n",
      "   -------- ------------------------------- 1.3/6.3 MB 828.3 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 1.3/6.3 MB 828.3 kB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 830.6 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 831.8 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 831.8 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.1/6.3 MB 838.7 kB/s eta 0:00:05\n",
      "   ------------- -------------------------- 2.1/6.3 MB 838.7 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.4/6.3 MB 784.9 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.4/6.3 MB 784.9 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 770.3 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 770.3 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 2.9/6.3 MB 745.6 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 2.9/6.3 MB 745.6 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 3.1/6.3 MB 753.3 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 3.1/6.3 MB 753.3 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 3.1/6.3 MB 753.3 kB/s eta 0:00:05\n",
      "   --------------------- ------------------ 3.4/6.3 MB 706.4 kB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 708.1 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 708.1 kB/s eta 0:00:04\n",
      "   ------------------------- -------------- 3.9/6.3 MB 705.4 kB/s eta 0:00:04\n",
      "   ------------------------- -------------- 3.9/6.3 MB 705.4 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 4.2/6.3 MB 693.3 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 4.2/6.3 MB 693.3 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 4.2/6.3 MB 693.3 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 683.0 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 683.0 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 683.0 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 4.7/6.3 MB 648.2 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 4.7/6.3 MB 648.2 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 4.7/6.3 MB 648.2 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 4.7/6.3 MB 648.2 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.0/6.3 MB 610.0 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.0/6.3 MB 610.0 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.0/6.3 MB 610.0 kB/s eta 0:00:03\n",
      "   --------------------------------- ------ 5.2/6.3 MB 606.0 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.3 MB 606.0 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 602.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 602.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 602.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 602.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 602.4 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 602.4 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.8/6.3 MB 551.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 551.3 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 551.3 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 543.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 474.9 kB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.20.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 540.5 kB/s eta 0:00:09\n",
      "   --- ------------------------------------ 0.5/5.4 MB 540.5 kB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 550.1 kB/s eta 0:00:09\n",
      "   ------- -------------------------------- 1.0/5.4 MB 621.2 kB/s eta 0:00:07\n",
      "   ------- -------------------------------- 1.0/5.4 MB 621.2 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 1.3/5.4 MB 645.3 kB/s eta 0:00:07\n",
      "   --------- ------------------------------ 1.3/5.4 MB 645.3 kB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 650.2 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 650.2 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.8/5.4 MB 653.5 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 1.8/5.4 MB 653.5 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 2.1/5.4 MB 652.3 kB/s eta 0:00:06\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 661.1 kB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 661.1 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 2.6/5.4 MB 668.0 kB/s eta 0:00:05\n",
      "   ------------------- -------------------- 2.6/5.4 MB 668.0 kB/s eta 0:00:05\n",
      "   --------------------- ------------------ 2.9/5.4 MB 665.7 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 2.9/5.4 MB 665.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 673.4 kB/s eta 0:00:04\n",
      "   ------------------------- -------------- 3.4/5.4 MB 682.4 kB/s eta 0:00:03\n",
      "   ------------------------- -------------- 3.4/5.4 MB 682.4 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 3.7/5.4 MB 677.2 kB/s eta 0:00:03\n",
      "   --------------------------- ------------ 3.7/5.4 MB 677.2 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 686.8 kB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 686.8 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 4.2/5.4 MB 683.9 kB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.2/5.4 MB 683.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.5/5.4 MB 677.8 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.5/5.4 MB 677.8 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 4.5/5.4 MB 677.8 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 657.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 657.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 657.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 657.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 657.1 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 657.1 kB/s eta 0:00:02\n",
      "   ------------------------------------ --- 5.0/5.4 MB 579.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 579.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 579.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 571.2 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 571.2 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 561.5 kB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-win_amd64.whl (150 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, smart-open, shellingham, pydantic-core, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, annotated-types, srsly, pydantic, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.20.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.1 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45b1d8db-3d3e-453b-b508-614cc43449ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
      "     - ------------------------------------- 0.5/12.8 MB 932.9 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.8/12.8 MB 884.1 kB/s eta 0:00:14\n",
      "     -- ------------------------------------ 0.8/12.8 MB 884.1 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.0/12.8 MB 812.4 kB/s eta 0:00:15\n",
      "     --- ----------------------------------- 1.3/12.8 MB 838.9 kB/s eta 0:00:14\n",
      "     --- ----------------------------------- 1.3/12.8 MB 838.9 kB/s eta 0:00:14\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 839.1 kB/s eta 0:00:14\n",
      "     ---- ---------------------------------- 1.6/12.8 MB 839.1 kB/s eta 0:00:14\n",
      "     ----- --------------------------------- 1.8/12.8 MB 818.3 kB/s eta 0:00:14\n",
      "     ------ -------------------------------- 2.1/12.8 MB 827.3 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.4/12.8 MB 838.9 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.4/12.8 MB 838.9 kB/s eta 0:00:13\n",
      "     ------- ------------------------------- 2.6/12.8 MB 829.9 kB/s eta 0:00:13\n",
      "     -------- ------------------------------ 2.9/12.8 MB 834.9 kB/s eta 0:00:12\n",
      "     -------- ------------------------------ 2.9/12.8 MB 834.9 kB/s eta 0:00:12\n",
      "     --------- ----------------------------- 3.1/12.8 MB 842.7 kB/s eta 0:00:12\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 846.1 kB/s eta 0:00:12\n",
      "     ---------- ---------------------------- 3.4/12.8 MB 846.1 kB/s eta 0:00:12\n",
      "     ----------- --------------------------- 3.7/12.8 MB 842.1 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.7/12.8 MB 842.1 kB/s eta 0:00:11\n",
      "     ----------- --------------------------- 3.9/12.8 MB 836.0 kB/s eta 0:00:11\n",
      "     ------------ -------------------------- 4.2/12.8 MB 827.8 kB/s eta 0:00:11\n",
      "     ------------ -------------------------- 4.2/12.8 MB 827.8 kB/s eta 0:00:11\n",
      "     ------------- ------------------------- 4.5/12.8 MB 823.5 kB/s eta 0:00:11\n",
      "     ------------- ------------------------- 4.5/12.8 MB 823.5 kB/s eta 0:00:11\n",
      "     -------------- ------------------------ 4.7/12.8 MB 808.1 kB/s eta 0:00:11\n",
      "     --------------- ----------------------- 5.0/12.8 MB 805.4 kB/s eta 0:00:10\n",
      "     --------------- ----------------------- 5.0/12.8 MB 805.4 kB/s eta 0:00:10\n",
      "     --------------- ----------------------- 5.2/12.8 MB 803.0 kB/s eta 0:00:10\n",
      "     --------------- ----------------------- 5.2/12.8 MB 803.0 kB/s eta 0:00:10\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 789.6 kB/s eta 0:00:10\n",
      "     ---------------- ---------------------- 5.5/12.8 MB 789.6 kB/s eta 0:00:10\n",
      "     ----------------- --------------------- 5.8/12.8 MB 790.0 kB/s eta 0:00:09\n",
      "     ------------------ -------------------- 6.0/12.8 MB 788.7 kB/s eta 0:00:09\n",
      "     ------------------ -------------------- 6.0/12.8 MB 788.7 kB/s eta 0:00:09\n",
      "     ------------------- ------------------- 6.3/12.8 MB 792.4 kB/s eta 0:00:09\n",
      "     ------------------- ------------------- 6.6/12.8 MB 795.8 kB/s eta 0:00:08\n",
      "     ------------------- ------------------- 6.6/12.8 MB 795.8 kB/s eta 0:00:08\n",
      "     -------------------- ------------------ 6.8/12.8 MB 797.5 kB/s eta 0:00:08\n",
      "     --------------------- ----------------- 7.1/12.8 MB 797.4 kB/s eta 0:00:08\n",
      "     --------------------- ----------------- 7.1/12.8 MB 797.4 kB/s eta 0:00:08\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 794.7 kB/s eta 0:00:07\n",
      "     ---------------------- ---------------- 7.3/12.8 MB 794.7 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.6/12.8 MB 793.5 kB/s eta 0:00:07\n",
      "     ----------------------- --------------- 7.9/12.8 MB 504.7 kB/s eta 0:00:10\n",
      "     ----------------------- --------------- 7.9/12.8 MB 504.7 kB/s eta 0:00:10\n",
      "     ------------------------ -------------- 8.1/12.8 MB 508.4 kB/s eta 0:00:10\n",
      "     ------------------------ -------------- 8.1/12.8 MB 508.4 kB/s eta 0:00:10\n",
      "     ------------------------- ------------- 8.4/12.8 MB 511.4 kB/s eta 0:00:09\n",
      "     ------------------------- ------------- 8.4/12.8 MB 511.4 kB/s eta 0:00:09\n",
      "     -------------------------- ------------ 8.7/12.8 MB 515.2 kB/s eta 0:00:09\n",
      "     -------------------------- ------------ 8.7/12.8 MB 515.2 kB/s eta 0:00:09\n",
      "     --------------------------- ----------- 8.9/12.8 MB 519.9 kB/s eta 0:00:08\n",
      "     --------------------------- ----------- 8.9/12.8 MB 519.9 kB/s eta 0:00:08\n",
      "     --------------------------- ----------- 8.9/12.8 MB 519.9 kB/s eta 0:00:08\n",
      "     --------------------------- ----------- 9.2/12.8 MB 518.1 kB/s eta 0:00:08\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 525.7 kB/s eta 0:00:07\n",
      "     ---------------------------- ---------- 9.4/12.8 MB 525.7 kB/s eta 0:00:07\n",
      "     ----------------------------- --------- 9.7/12.8 MB 529.3 kB/s eta 0:00:06\n",
      "     ----------------------------- --------- 9.7/12.8 MB 529.3 kB/s eta 0:00:06\n",
      "     ----------------------------- -------- 10.0/12.8 MB 531.0 kB/s eta 0:00:06\n",
      "     ------------------------------ ------- 10.2/12.8 MB 538.0 kB/s eta 0:00:05\n",
      "     ------------------------------ ------- 10.2/12.8 MB 538.0 kB/s eta 0:00:05\n",
      "     ------------------------------- ------ 10.5/12.8 MB 542.6 kB/s eta 0:00:05\n",
      "     ------------------------------- ------ 10.7/12.8 MB 544.7 kB/s eta 0:00:04\n",
      "     ------------------------------- ------ 10.7/12.8 MB 544.7 kB/s eta 0:00:04\n",
      "     -------------------------------- ----- 11.0/12.8 MB 546.8 kB/s eta 0:00:04\n",
      "     -------------------------------- ----- 11.0/12.8 MB 546.8 kB/s eta 0:00:04\n",
      "     --------------------------------- ---- 11.3/12.8 MB 550.1 kB/s eta 0:00:03\n",
      "     --------------------------------- ---- 11.3/12.8 MB 550.1 kB/s eta 0:00:03\n",
      "     ---------------------------------- --- 11.5/12.8 MB 552.8 kB/s eta 0:00:03\n",
      "     ----------------------------------- -- 11.8/12.8 MB 556.7 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 11.8/12.8 MB 556.7 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 12.1/12.8 MB 560.5 kB/s eta 0:00:02\n",
      "     ----------------------------------- -- 12.1/12.8 MB 560.5 kB/s eta 0:00:02\n",
      "     ------------------------------------ - 12.3/12.8 MB 561.7 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 566.5 kB/s eta 0:00:01\n",
      "     -------------------------------------  12.6/12.8 MB 566.5 kB/s eta 0:00:01\n",
      "     -------------------------------------- 12.8/12.8 MB 566.2 kB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a8270f-cabc-4b75-ba5d-e1a43bab5473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello INTJ ROOT\n",
      ", PUNCT punct\n",
      "world NOUN npadvmod\n",
      "! PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello, world!\")\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a009568f-6eda-48c4-978a-b4e70f0a8617",
   "metadata": {},
   "source": [
    "## 3. Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94104a9-074b-4e1d-8491-68b8bd64bef8",
   "metadata": {},
   "source": [
    "- Gensim is a library for topic modeling and document similarity analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "34edc62a-3ac5-4126-aa33-a9b798e40792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5dee0b7-86d1-4d00-8097-4385dd2cf10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Using cached pip-25.0.1-py3-none-any.whl (1.8 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\devad\\AppData\\Local\\Programs\\Python\\Python312\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e708ccd-3b5f-477f-b9a7-1a022dd8a6e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Downloading numpy-2.2.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: gensim in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.3)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\devad\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy gensim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d25461f-18ec-460e-958b-fca7fdee70ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.7274825e-03  2.1301615e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281426e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986930e-03 -6.8730675e-03 -4.9994122e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033178e-03 -2.7436293e-03 -8.3628409e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441375e-03 -1.7069972e-03\n",
      " -8.9569986e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668312e-03  3.1185520e-03 -9.5707225e-03\n",
      "  1.4764392e-03  6.5244664e-03  5.7464195e-03 -8.7630618e-03\n",
      " -4.5171441e-03 -8.1401607e-03  4.5956374e-05  9.2636338e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610625e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703874e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161034e-03 -5.9258583e-04  7.4888230e-03\n",
      " -6.9751858e-04 -1.6249407e-03  2.7443992e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361041e-03 -9.5840869e-03  2.4462664e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669187e-03 -7.7365171e-03\n",
      "  8.3959233e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430846e-03  2.6350426e-03  7.4271322e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583735e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856887e-03  5.7358947e-03 -7.7733636e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061326e-03  2.6675343e-04  3.8572443e-03\n",
      "  7.3857834e-03 -6.7251669e-03  5.5844807e-03 -9.5222248e-03\n",
      " -8.0445886e-04 -8.6887367e-03 -5.0986730e-03  9.2892265e-03\n",
      " -1.8582619e-03  2.9144264e-03  9.0712793e-03  8.9381328e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044310e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758976e-03]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "sentences = [[\"hello\", \"world\"], [\"my\", \"name\", \"is\", \"Vamsi\"]]\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "vector = model.wv['hello']\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90656a5-588d-4aa1-915b-41d666c78811",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
